{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2797c9-8b91-40fd-8b9a-65e75e039550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:104: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=th.float16)\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:128: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dZ):\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:177: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=th.float16)\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:207: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, *dZ):\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:287: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=th.float16)\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:304: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dZ):\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:352: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=th.float16)\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:371: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, *dZ):\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:431: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=th.float16)\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:467: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_out):\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:498: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=th.float16)\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:535: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, *grad_out):\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:566: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=th.float16)\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:575: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dy):\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:595: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=th.float16)\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:603: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dy):\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:666: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=th.float16)\n",
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:692: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=th.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells  4992   genes 3004   graph edges 29952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muscede2/anaconda3/envs/spaformer/lib/python3.10/site-packages/dgl/backend/pytorch/tensor.py:352: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), \"Cannot convert view \" \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent embeddings: torch.Size([4992, 256])\n",
      "✓ saved to Data/spaformer_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# ░░░  Cell: SpaFormer embedding generation  ░░░\n",
    "import os, sys, numpy as np, torch, dgl\n",
    "sys.path.append('.')                           # make \"spaformer/\" visible\n",
    "\n",
    "from spaformer.edcoder import PreModel\n",
    "\n",
    "# -- 1. load prepared arrays -------------------------------------------------\n",
    "X      = np.load(\"Data/spaformer_prepared/X.npy\").astype('float32')   # (N,G)\n",
    "edges  = np.load(\"Data/spaformer_prepared/edges.npy\").astype('int64') # (2,E)\n",
    "\n",
    "print(f\"Cells {X.shape[0]:>5}   genes {X.shape[1]:>4}   graph edges {edges.shape[1]}\")\n",
    "\n",
    "# -- 2. build a DGL graph -----------------------------------------------------\n",
    "g = dgl.graph((edges[0], edges[1]), num_nodes=X.shape[0])\n",
    "g = dgl.add_self_loop(g)                # encoders usually expect self-loops\n",
    "device = torch.device('cpu')\n",
    "g = g.to(device)\n",
    "\n",
    "# store features inside the graph for encoders that look them up there\n",
    "g.ndata['feat'] = torch.from_numpy(X).to(device)\n",
    "\n",
    "# -- 3. instantiate **encoder-only** SpaFormer -------------------------------\n",
    "model = PreModel(\n",
    "        in_dim         = X.shape[1],\n",
    "        num_hidden     = 256,\n",
    "        num_layers     = 4,\n",
    "        nhead          = 8,\n",
    "        nhead_out      = 8,\n",
    "        activation     = 'gelu',\n",
    "        feat_drop      = 0.1,\n",
    "        attn_drop      = 0.1,\n",
    "        negative_slope = 0.2,\n",
    "        norm           = 'layernorm',\n",
    "\n",
    "        encoder_type   = 'gin',     # lightweight, no extra kwargs needed\n",
    "        decoder_type   = 'linear',  # stub – we ignore the decoder\n",
    "        loss_fn        = 'mse',\n",
    "        latent_dim     = 256,\n",
    "\n",
    "        mask_node_rate     = 0.0,\n",
    "        mask_feature_rate  = 0.0,\n",
    "        objective      = 'ae',\n",
    ").to(device).eval()\n",
    "\n",
    "# -- 4. (optional) load checkpoint & discard decoder -------------------------\n",
    "ckpt = \"checkpoints/spaformer_encoder_pretrain.pth\"\n",
    "if os.path.exists(ckpt):\n",
    "    state = torch.load(ckpt, map_location=device)\n",
    "    state = {k.replace('module.', ''): v            # strip DDP prefixes\n",
    "             for k, v in state.items()\n",
    "             if not k.startswith('decoder')}        # drop decoder weights\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    print(\"✓ encoder checkpoint loaded\")\n",
    "\n",
    "# -- 5. forward pass (embedding extraction) ----------------------------------\n",
    "with torch.no_grad():\n",
    "    z = model.embed(                               # << use .embed(...)\n",
    "        g,\n",
    "        torch.from_numpy(X).to(device)             # node features\n",
    "    )                                              # → (N, latent_dim)\n",
    "\n",
    "print(\"Latent embeddings:\", z.shape)\n",
    "\n",
    "# -- 6. save -----------------------------------------------------------------\n",
    "out_path = \"Data/spaformer_embeddings.npy\"\n",
    "np.save(out_path, z.cpu().numpy())\n",
    "print(\"✓ saved to\", out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spaformer)",
   "language": "python",
   "name": "spaformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
